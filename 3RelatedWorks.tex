\chapter{Related Works}
\label{chapter:related}
In this chapter, we discuss the related works about texture reconstruction in section~\ref{related:texture-recon}. We dusciss seamless surface parameterization in section~\ref{related:param} as a classical problem in geometry processing community. We finally discuss related works about texture understanding,  including semantics understanding from parameterized texture (section~\ref{related:texturenet}) and canonical frame understanding from 2D images (section~\ref{related:framenet}).

\section{Texture Reconstruction}
\label{related:texture-recon}

\subsection{Geometry Priors}
Many computer graphics applications use approximated geometry priors with high-quality textures to create a real-time and appealing visual effect. Therefore, our texture reconstruction is optionally performed with the geometry approximation using abstract primitives or CAD models.

\paragraph*{Imposters} The idea of approximating distant geometry through images called {\em impostors} was widely used in the image-based rendering literature~\cite{sillion1997efficient,decoret1999multi}. 
These images are actually textured planes optimally positioned to generate approximately correct views from a range of viewpoints.
Image-based rendering has also been recently used in custom renderers with a geometry proxy and achieved high-quality results using sparse DSLR input images \cite{hedman2016scalable}.

\paragraph*{Primitive Abstraction} Since man-made environments are typically constructed in a highly-structured fashion, with an abundance of orthogonal and parallel planes, a Manhattan plane assumption can be exploited to facilitate tracking in indoor scenes, particularly in the case of RGB-D scanning, as depth information is directly available.
Many methods have been developed to incorporate planar information to improve camera tracking in 3D scanning scenarios.
Dou et al.~\cite{dou2012exploring} and Taguchi et al.~\cite{taguchi2013point} both incorporate various plane correspondences with feature point correspondences to improve tracking robustness.
Zhang et al.~\cite{zhang2015online} detect both planar structures and repeated objects to mitigate camera drift, achieving improved reconstruction quality in a KinectFusion-style framework.
In order to reduce sensitivity towards potential errors in initially detected structural correspondences, Halber and Funkhouser~\cite{halber2016fine} employ a hierarchical optimization approach incorporating planar relationship constraints with sparse features, enabling registration of very long RGB-D scanning sequences.
In addition to improving tracking robustness, Dzitsiuk et al.~\cite{dzitsiuk2016noising} use plane priors estimated directly on the implicit signed distance field representation of the scene to further de-noise and complete the reconstruction, producing very stable geometry even during real-time updates.
Surface measurements near detected planes are replaced with the plane measurements to reduce noise, and plane geometry is extrapolated in unobserved space to fill holes.

\paragraph*{CAD Model Replacement} Recently, we are witnessing growing efforts to replace the observed noisy, cluttered and partial scans with clean geometry, such as artist-created CAD models~\cite{Li:2015,Avetisyan:2019,Avetisyan:2019:Scan2CAD,Dahnert:2019}. In this way, eventually, an entire scene can be virtualized into a set of 3D models that are free of noise, partiality, and scanning artifacts -- while maintaining the semantically valid structure and realistic appearance. One straightforward way to achieve this goal is to replace the sensor data by a known CAD model \textit{retrieved} from an existing repository.

\paragraph*{Texture Reconstruction with Geometry Priors} Our approaches similarly exploit planes and CAD model replacement as a clean geometric representation of a 3D scene or an object. Different from existing methods that focus on geometry cleaning, we aim at creating visually compelling models by generating high-quality texture on top of the geometry priors.

\subsection{Texture Optimization}
\paragraph*{Parametric Color Optimization}
Several approaches have been proposed to improve the mapping of input images to textures with parametric models,
leveraging both human supervision~\cite{franken2005minimizing,ofek1997multiresolution,neugebauer1999texturing,rocchini1999multiple,stamos20003d,pighin2006synthesizing,xu2019deep} as well as automatic optimization~\cite{bernardini2001high,pulli2000surface}. 
Given precisely registered pairs of color and depth, camera poses can be optimized for to maximize photo-consistency~\cite{johnson1999registration,pulli2000surface,pulli2005projective,bernardini2001high}. For 3D scanning using commodity-sensor data with various misalignments arising from coarse geometry and optical irregularities, Zhou and Koltun~\cite{zhou2014color} account for these issues by optimizing for both the rigid camera poses as well as non-rigid warping for each image to maximize dense photo-consistency.

Our first approach, \emph{3DLite}~\cite{huang20173dlite}, also builds upon the approach of Zhou et al.~\cite{zhou2014color}; however, for our room-scale scenario, optimizing purely for a dense energy term remains sensitive to initial poses and easy to end up in local minima. 
Rather, we employ a sparse-to-dense optimization, using sparse color features and geometric primitive constraints to help reach the basin of convergence of the dense photo-consistency energy.
While these methods are able to fix small misalignments, their deformation models are often not expressive enough to handle many real-world distortions, particularly those due to largely approximate surface geometry. 
In contrast to a hand-crafted deformation model, our second approach, \emph{Adversarial Texture Optimization}~\cite{huang2020adversarial}, learns a distortion-tolerant adversarial loss.

\paragraph*{Patch-based Color Optimization}
Patch-based image synthesis strategies have been proposed for color texture optimization~\cite{bi2017patch}. Rather than non-rigid image warping, they re-synthesize the input image with the nearest patch search~\cite{simakov2008summarizing} to handle misalignments. 
However, general misalignment cannot be accurately modeled by translating patches, and the L2 loss is not robust to color, lighting or sharpness differences. Our first approach~\cite{huang20173dlite} instead compensates the lighting errors with explicit parametric models, while our second method~\cite{huang2020adversarial} optimizes the discriminator to cover all these problems without requiring explicit re-synthesis.

\paragraph*{View Aggregation} Common texture reconstruction methods~\cite{izadi2011kinectfusion,zhou2014color} average projected input images to generate textures.  
To reduce blurriness artifacts caused by average aggregation, some approaches select a single or a few candidate views for each region \cite{dessein2014seamless}. Others formulate a multi-label selection energy minimization problem to minimize seam artifacts~\cite{lempitsky2007seamless,sinha2008interactive,velho2007projective,waechter2014let}. Our first approach~\cite{huang20173dlite} further takes pixel sharpness into consideration given commonly existing motion blur in the commodity sensors. We aim at selecting the best view for each region to balance the visual sharpness and color consistency of boundaries between neighboring regions with different views selected, which is modeled as a multi-label graph-cut problem~\cite{boykov2001fast}. Our second method~\cite{huang2020adversarial} does not explicitly define the aggregation method, but implicitly aggregates colors from different views based on a learned adversarial metric.

\paragraph*{Neural Textures}
Recently, neural rendering approaches have been proposed to synthesize a feature map on a surface that can be interpreted by a deep network to produce novel image views.  For instance, \cite{thies2019deferred} stores appearance information as high-dimensional features in a neural texture map associated with the coarse geometry proxy and decodes to color when projected to novel views.  \cite{sitzmann2019deepvoxels} stores the appearance information as high-dimensional features in volumes, and \cite{aliev2019neural} uses features stored with points. These methods rely on the representation power of generative networks at rendering times to obtain novel viewpoints, which limits their applicability in standard graphics pipelines. Further, there is no guarantee that the trained neural textures are robust to novel views far from the training viewpoints. In comparison, our second method~\cite{huang2020adversarial} uses the neural architecture as a deep metric and optimizes for the low-dimensional RGB textures, which is friendly to traditional graphics pipelines and guarantees consistent novel viewpoint rendering.

\section{Surface Parameterization}
\label{related:param}
The seamless surface parameterization problem is closely related to the classical quadrangulation problem in the geometry processing community which has been studied for decades. We tackle the quadrangulation problem as a fundamental step for texture optimization.

\paragraph*{Robust Quadrangulations}
Robust quadrangulations based on local optimization have a rich literature. Bommes et al.~\cite{bommes2013quad} survey many existing methods for quad-mesh generation and processing.  Many methods transform a pre-existing triangle mesh into an all-quadrilateral mesh: Q-Morph~\cite{owen1999q} does so with an advancing front algorithm.  Blossom-Quad~\cite{remacle2012blossom} uses a perfect matching algorithm to pair triangles into quads with a global-optimal solution. Velho and Zorin~\cite{velho20014} greedily identify the most eligible neighboring triangles to pair. SQuad~\cite{gurung2011squad} improves the representation of the connectivity of meshes, which can be applied to quadrangulation.  Since these methods are not guided by an orientation field, they have difficulty achieving global regularity or smoothness, and their meshes are often very irregular or have many singularities.  Spectral and Morse complex-based algorithms \cite{dong2006spectral,zhang2010wave,ling2014spectral} require no integer optimization but singularity control is hard for them. Such irregularities cause problems for regular convolutions in our texture understanding setting.

\paragraph*{Orientation Fields}
An orientation field is a powerful tool to guide the edge directions in a quad mesh. Specifically, we use 4-way rotationally symmetric orientation fields~\cite{ray2008n,lai2010metric}. The target directions are derived from the principal curvatures~\cite{cohen2003restricted,cazals2005estimating}, but modified to vary smoothly. Smooth orientation fields are generated by optimizing a nonlinear energy function based on periodic functions~\cite{hertzmann2000illustrating,ray2009geometry} or a mixed-integer representation~\cite{ray2008n,bommes2009mixed}. However, these approaches may get stuck in poor local minima that have many singularities. Kn\"{o}ppel et al.~\cite{knoppel2013globally} propose a global optimization method to obtain better minima. Many approaches \cite{ray2008n,ray2009geometry,crane2010trivial,diamanti2014designing,jiang2015frame} integrate user interactions to help remove singularities. Our approach~\cite{huang2018quadriflow} is guided by the pre-computed orientation field to enforce minimum singularities in the parameterization.

\paragraph*{Field-aligned Quadrangulations}
A direct approach to generating a quad mesh is to trace the curves in an orientation field~\cite{alliez2003anisotropic}, but it is difficult to control the sizes of the quads obtained that way. Lai et al.~\cite{lai2008incremental} directly optimize a triangle mesh to align its edges to an orientation field, then extract a quad mesh by pairing triangles. These methods are local, so they generate many unnecessary singularities.

Another line of work that deals with a global parameterization of maps is based on global optimization that explicitly bounds the map distortion and produces injective maps. The parameterization can be extracted as quad meshes using \texttt{libQEx}~\cite{ebke2013qex}. Levi and Zorin~\cite{levi2014strict} achieve minimum worst-case distortion and prioritize higher distortion reduction. Chien et al.~\cite{chien2016bounded} solve a locally injective map efficiently with sequential convex programming. Myles et al.~\cite{myles2014robust} use cross-field line tracing to initialize the quad patch partition, then compute a bijective global parametrization.

Global methods aim at jointly optimizing the parametrization with integer constraints that are usually not polynomial-time tractable. The objective is typically represented as a MIP problem \cite{bommes2009mixed}. The numerical solvers are designed to enforce low distortion and reduce the number of singularities \cite{bommes2013integer,myles2013controlled,levi2014strict,myles2014robust}. Another global integrable approach~\cite{diamanti2015integrable} minimizes nonlinear energy, but the optimization process is still challenging. The output of the global methods is called the Integer-Grid Map (IGM)~\cite{bommes2013integer}, where the final quad mesh can be extracted by \texttt{libQEx}~\cite{ebke2013qex}. These methods have full control of edge alignment and singularity placement and usually generate very high-quality quad meshes. However, their implementations are complex, and usually not scalable. Quantized Global Optimization \cite{Campen2015QGP} uses motorcycle graphs to quickly construct a valid quantization based on a seamless parametrization from a MIP solver. Instead, we derive a mathematical approximation to the fundamental MIP problem with a minimum cost flow graph and guarantee the end-to-end quadrangulation from the orientation field with a polynomial-time solution.

\section{Texture Understanding}
\subsection{Semantics from Texture}
\label{related:texturenet}
\paragraph*{3D Deep Learning.}
With the availability of 3D shape databases \cite{wu20153d,chang2015shapenet,song2017semantic} and real-world labeled 3D scanning data \cite{song2015sun,armeni2017joint,dai2017scannet,chang2017matterport3d}, there is significant interest in deep learning on three-dimensional data.   Early work developed CNNs operating on 3D volumetric grids \cite{wu20153d,maturana2015voxnet}.  They have been used for 3D shape classification  \cite{qi2016volumetric,riegler2017octnet}, semantic  segmentation \cite{dai2017scannet,dai2018scancomplete}, object completion \cite{dai2017shape}, and scene completion \cite{dai2018scancomplete}.   More recently, researchers have developed methods that can take a 3D point cloud as input to a neural network and predict object classes or semantic point labels \cite{qi2017pointnet,qi2017pointnet++,tatarchenko2018tangent,su2018splatnet,atzmon2018point}.  AtlasNet~\cite{groueix2018papier} learns to generate surfaces of the 3D shape.  In our work, we utilize a sparse point sampled data representation, however, we exploit high resolution signals on geometric surface structures with a new 4-RoSy surface convolution kernel.

\paragraph*{Convolutions on Meshes.}
Several researchers have proposed methods for applying convolutional neural networks intrinsically on manifold meshes.  FeaStNet~\cite{verma2018feastnet} proposes a graph operator that establishes correspondences between filter weights. Jiang \textit{et al.}~\cite{jiang2019spherical} applies differential operators on unstructured spherical grids.
GCNN~\cite{masci2015geodesic} proposes using discrete patch operators on tangent planes parameterized by radius and angles. 
However, the orientation of their selected geodesic patches is arbitrary, and the parameterization is highly distorted or inconsistent at regions with high Gaussian curvature. 
ACNN~\cite{boscaini2016learning} observes this limitation and introduces the anisotropic heat kernels derived from principal curvatures. MoNet~\cite{monti2017geometric} further generalizes the architecture with the learnable gaussian kernels for convolutions.
The principal curvature based frame selection method is adopted by Xu \textit{et al.}~\cite{xu2017directionally} for segmentation of nonrigid surfaces, by Tatarchenko \textit{et al.}~\cite{tatarchenko2018tangent} for semantic segmentation of point clouds, and by ADD~\cite{boscaini2016anisotropic} for shape correspondence in the spectral domain. 
It naturally removes orientation ambiguity but fails to consider frame inconsistency problem, which is critical when performing feature aggregation.  Its problems are particularly pronounced in indoor scenes (which often have many planar regions where principal curvatures are undetermined) and in real-world scans (which often have noisy and uneven sampling where consistent principal curvatures are difficult to predict).   In contrast, we define a 4-RoSy field that provides consistent orientations for neighboring convolution domains.

\paragraph*{Multi-view and 2D-3D Joint Learning.}
Other researchers have investigated how to incorporate features from RGB inputs to 3D deep networks.  The typical approach is to simply assign color values to voxels, points, or mesh vertices and treat them as additional feature channels.
However, given that geometry and RGB data are at vastly different resolutions, this approach leads to significant downsampling of the color signal and thus does not take full advantage of the high-frequency patterns therein.   An alternative approach is to combine features extracted from RGB images in a multi-view CNN \cite{su2015multi}. This approach has been used for 3D semantic segmentation in 3DMV \cite{dai20183dmv}, where features are extracted from 2D RGB images and then back-projected into a 3D voxel grid where they are merged and further processed with 3D voxel convolutions.  Like our approach, 3DMV processes high-resolution RGB signals; however it convolves them in a 2D image plane, where occlusions and background clutter are confounding.  In contrast, our method directly convolves high-resolution signals intrinsically on the 3D surface which is view-independent.

\subsection{Surface Parameterization from Texture}
\label{related:framenet}
\paragraph*{3D from Single Image.}
Estimating 2.5D geometry properties from a single image has become popular in recent years. Traditional methods aim at understanding low-level image information and geometry constraints. For example, Torralba \textit{et al.}~\cite{torralba2002depth} exploits the scene structure to estimate the absolute depth values. Saxena \textit{et al.}~\cite{saxena2006learning} uses hand-crafted features to predict the depth based on Markov random fields. Hoiem \textit{et al.}~\cite{hoiem2007recovering} recovers scene layout guided by the vanishing points and lines. Shi \textit{et al.}~\cite{shi2015break} estimates the defocus blur and uses it to assist depth estimation.

With the availability of large-scale datasets and the success of deep learning, many methods have been proposed for depth or/and normal estimation. For depth estimation, Eigen \textit{et al.}~\cite{eigen2014depth} uses CNN to predict indoor depth maps on the NYUv2 dataset. With the powerful backbone network like VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}, depth estimation can be further improved~\cite{garg2016unsupervised,xie2016deep3d}. DORN~\cite{fu2018deep} proposes a novel ordinary loss and achieves the state-of-the-art in KITTI~\cite{geiger2013vision}. For surface normal estimation, Wang \textit{et al.}~\cite{wang2015designing} incorporate vanishing point and layout information in the network architecture. Eigen and Fergus~\cite{eigen2015predicting} trained a coarse-to-fine CNN to refine the details of the normals. The skip-connected architecture~\cite{bansal2016marr} is proposed to fuse hidden layers for normal estimation.

Since surface normal and depth are related to each other, another set of methods aimed at jointly predicting both to improve the performance. Wang \textit{et al.}~\cite{wang2016surge} exploits the consistency between normal and depth in planar regions. GeoNet~\cite{qi2018geonet} proposes a refinement network to enhance the depth and normal estimation from each other. Zhang \textit{et al.}~\cite{zhang2018deep} predict the normal and solve a global optimization problem to complete the depth.  We take a further step by jointly estimating all axes of a 3D canonical frame at each pixel, which helps both regularize the prediction through constraints and is useful in applications.

\paragraph*{Local Canonical Frames}
\jw{Computing local \cframe{} on surfaces is a fundamental step for many problems.} 3DLite~\cite{huang20173dlite} builds \cframe{} in fitted 3D planes for color optimizations. GCNN~\cite{masci2015geodesic} defines local frames with spherical coordinates and applies discrete patch operators on tangent planes. ACNN~\cite{boscaini2016learning} introduces the anisotropic heat kernels derived from principal curvatures \jw{so that it can apply convolutions in canonical frames defined by principal axes. Such canonical frame} is also used in Xu \textit{et al.}~\cite{xu2017directionally} for nonrigid segmentation, by Tatarchenko \textit{et al.}~\cite{tatarchenko2018tangent,huang2018texturenet} for semantic segmentation of the 3D scenes. \jw{We aim at recognizing such frames from 2D images, and compute them from 3D surfaces to supervise the learning.}

TextureNet~\cite{huang2018texturenet} highlights the challenges of computing robust local \cframe{} at planar surface regions, where the principal curvatures are undetermined or highly influenced by noise or uneven sampling. Therefore, it proposes to compute a 4-RoSy orientation field to represent the principal directions. The 4-RoSy orientation field is an important concept in the geometry processing community~\cite{ray2008n,lai2010metric}. The target directions are aligned with the principal curvatures~\cite{cohen2003restricted,cazals2005estimating}, but regularized by additional energy to vary smoothly. This can be achieved by optimizing a nonlinear energy by periodic functions~\cite{hertzmann2000illustrating,ray2009geometry} or a mixed-integer representation~\cite{ray2008n,bommes2009mixed}. In our work, we use QuadriFlow~\cite{huang2018quadriflow} to optimize the 4-RoSy field so that it aligns with the principal curvatures at the curved surface and ensures smoothness in flat regions (where principal directions are ill-defined), as well as robustness to noise.
